# -*- coding: utf-8 -*-
"""sriramBioMistral_AI.ipynb à°•à°¾à°ªà±€ à°•à°¾à°ªà±€

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1s4h9FSvHN5Z4nbLwSRpd60Jf1fgXU0ln
"""

#pip install -U openai-whisper

"""BlenderBot-Encoder-**Decoder**

"""

#pip install --upgrade --no-deps --force-reinstall git+https://github.com/openai/whisper.git

#pip install git+https://github.com/openai/whisper.git

!pip install langchain

!pip install langchain_community # Install the missing langchain_community package.
!pip install langchain_community

from langchain.llms import HuggingFacePipeline
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline

# Define model id
model_id = 'facebook/blenderbot-1B-distill'

# Load tokenizer and model
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForSeq2SeqLM.from_pretrained(model_id)

# Create a pipeline
pipe = pipeline(
    "text2text-generation",
    model=model,
    tokenizer=tokenizer,
    max_length=100
)

# Initialize local LLM with HuggingFacePipeline
local_llm = HuggingFacePipeline(pipeline=pipe)

# Define the prompt
prompt = PromptTemplate(template="{question}", input_variables=["question"])

# Create the LLM chain
llm_chain = LLMChain(prompt=prompt, llm=local_llm)

# Define the question
question = "Tell me about you?"

# Run the chain with the question and print the result
print(llm_chain.run(question))

!pip install langchain_community # Install the missing langchain_community package.
!pip install langchain_community

from langchain.llms import HuggingFacePipeline
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline

# Define model id
model_id = 'nikhil928/facebook-blenderbot-1B-distill'

# Load tokenizer and model
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForSeq2SeqLM.from_pretrained(model_id)

# Create a pipeline
pipe = pipeline(
    "text2text-generation",
    model=model,
    tokenizer=tokenizer,
    max_length=100
)

# Initialize local LLM with HuggingFacePipeline
local_llm = HuggingFacePipeline(pipeline=pipe)

# Define the prompt template
prompt = PromptTemplate(template="{question}", input_variables=["question"])

# Create the LLM chain
llm_chain = LLMChain(prompt=prompt, llm=local_llm)

# Start the interactive loop
while True:
    # Get user input
    question = input("Ask a Question: ")

    # Check if the user wants to exit
    if question.lower() == 'exit':
        print("Exiting the conversation.")
        break

    # Run the chain with the question and print the result
    response = llm_chain.run(question)
    print(f"Response: {response}\n")

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# Load the model and tokenizer
model_name = "tiiuae/falcon-7b-instruct"  # Ensure the correct model name
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Move the model to GPU if available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Continuously generate text in a loop until the user types 'exit'
while True:
    # Define the prompt
    prompt = input("Enter a prompt (or type 'exit' to stop): ")

    # Exit the loop if the user types 'exit'
    if prompt.lower() == 'exit':
        print("Exiting the program.")
        break

    # Tokenize the prompt and move input to device
    input_ids = tokenizer(prompt, return_tensors="pt").input_ids.to(device)

    # Generate the output with controlled length
    outputs = model.generate(input_ids, max_new_tokens=100, do_sample=True, top_p=0.9)

    # Decode the generated text
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # Print the generated text
    print("\nGenerated text:\n", generated_text)
    print("\n" + "-"*50 + "\n")  # Separator for clarity

"""# Build BioMistral Medical RAG Chatbot using BioMistral Open Source LLM

In the notebook we will build a Medical Chatbot with BioMistral LLM and Heart Health pdf file.
"""

#shell
!pip install huggingface_hub

"""## Installation"""

!pip install langchain sentence-transformers chromadb llama-cpp-python langchain_community pypdf

"""## Import libraries"""

from langchain_community.document_loaders import PyPDFDirectoryLoader
from langchain.text_splitter import CharacterTextSplitter,RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS, Chroma
from langchain_community.llms import LlamaCpp
from langchain.chains import RetrievalQA, LLMChain

import pathlib
import textwrap
from IPython.display import display
from IPython.display import Markdown



def to_markdown(text):
  text = text.replace('â€¢', '  *')
  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))

# Used to securely store your API key
from google.colab import userdata

"""## Setup HuggingFace Access Token

- Log in to [HuggingFace.co](https://huggingface.co/)
- Click on your profile icon at the top-right corner, then choose [â€œSettings.â€](https://huggingface.co/settings/)
- In the left sidebar, navigate to [â€œAccess Tokenâ€](https://huggingface.co/settings/tokens)
- Generate a new access token, assigning it the â€œwriteâ€ role.

"""

# Or use `os.getenv('HUGGINGFACEHUB_API_TOKEN')` to fetch an environment variable.
import os
from getpass import getpass

HUGGINGFACEHUB_API_TOKEN = userdata.get("HUGGINGFACEHUB_API_TOKEN")
os.environ["HUGGINGFACEHUB_API_TOKEN"] = "HUGGINGFACEHUB_API_TOKEN"

"""## Import document"""

#connect to google drive
from google.colab import drive
drive.mount('/content/drive') #Fixed typo in directory name

loader = PyPDFDirectoryLoader("/content/drive/MyDrive/Pdf")
docs = loader.load()

docs

"""## Text Splitting - Chunking"""

text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)
chunks = text_splitter.split_documents(docs)

len(chunks)

chunks[0]

chunks[1]

chunks[2]

chunks[3]

chunks[4]

embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-base-en-v1.5")

"""## Embeddings

## Vector Store - FAISS or ChromaDB
"""

vectorstore = Chroma.from_documents(chunks, embeddings)

vectorstore

query = "Result" # what is at risk of heart disease
search = vectorstore.similarity_search(query)

to_markdown(search[0].page_content)

"""## Retriever"""

retriever = vectorstore.as_retriever(
    search_kwargs={'k': 5}
)

retriever.get_relevant_documents(query)

"""## Large Language Model - Open Source"""

drive.mount('/content/drive')

llm = LlamaCpp(
    model_path= "/content/drive/MyDrive/Pdf/Bio/BioMistral-7B.Q4_K_M.gguf",
    temperature=0.3,
    max_tokens=2048,
    top_p=1)

"""## RAG Chain"""

from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser
from langchain.prompts import ChatPromptTemplate

template = """
<|context|>
You are an AI assistant that follows instruction extremely well.
Please be truthful and give direct answers
</s>
<|user|>
{query}
</s>
 <|assistant|>
"""

prompt = ChatPromptTemplate.from_template(template)

rag_chain = (
    {"context": retriever,  "query": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)



response = rag_chain.invoke("what disease ?")

to_markdown(response)

import sys

while True:
  user_input = input(f"Input Prompt: ")
  if user_input == 'exit':
    print('Exiting')
    sys.exit()
  if user_input == '':
    continue
  result = rag_chain.invoke(user_input)
  print("Answer: ",result)

!pip install pyttsx3

import pyttsx3
import sys
from langchain_core._api.deprecation import surface_langchain_deprecation_warnings

# Initialize text-to-speech engine
engine = pyttsx3.init()

# Function to convert text to speech
def speak(text):
    engine.say(text)
    engine.runAndWait()

# Your existing code
from langchain_community.document_loaders import PyPDFDirectoryLoader
from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS, Chroma
from langchain_community.llms import LlamaCpp
from langchain.chains import RetrievalQA, LLMChain

import pathlib
import textwrap
from IPython.display import display, Markdown

def to_markdown(text):
    text = text.replace('â€¢', '  *')
    return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))

# Setup HuggingFace Access Token
import os
from getpass import getpass

HUGGINGFACEHUB_API_TOKEN = os.getenv("HUGGINGFACEHUB_API_TOKEN")
os.environ["HUGGINGFACEHUB_API_TOKEN"] = HUGGINGFACEHUB_API_TOKEN

# Import document
from google.colab import drive
drive.mount('/content/drive')

loader = PyPDFDirectoryLoader("/content/drive/MyDrive/Pdf")
docs = loader.load()

# Text Splitting - Chunking
text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)
chunks = text_splitter.split_documents(docs)

embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-base-en-v1.5")

# Vector Store - FAISS or ChromaDB
vectorstore = Chroma.from_documents(chunks, embeddings)

query = "Result"
search = vectorstore.similarity_search(query)
to_markdown(search[0].page_content)

# Retriever
retriever = vectorstore.as_retriever(search_kwargs={'k': 5})
retriever.get_relevant_documents(query)

# Large Language Model - Open Source
llm = LlamaCpp(
    model_path= "/content/drive/MyDrive/Pdf/Bio/BioMistral-7B.Q4_K_M.gguf",
    temperature=0.3,
    max_tokens=2048,
    top_p=1
)

# RAG Chain
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser
from langchain.prompts import ChatPromptTemplate

template = """
<|context|>
You are an AI assistant that follows instruction extremely well.
Please be truthful and give direct answers
</s>
<|user|>
{query}
</s>
 <|assistant|>
"""

prompt = ChatPromptTemplate.from_template(template)
rag_chain = (
    {"context": retriever, "query": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

# Interactive Loop
while True:
    user_input = input(f"Input Prompt: ")
    if user_input == 'exit':
        print('Exiting')
        speak('Exiting')
        sys.exit()
    if user_input == '':
        continue
    result = rag_chain.invoke(user_input)
    print("Answer: ", result)
    speak(result)

!sudo apt-get update
!sudo apt-get install espeak

!sudo apt-get install libespeak1

engine = pyttsx3.init(driverName='espeak')

import pyttsx3
import sys
from langchain_core._api.deprecation import surface_langchain_deprecation_warnings

# Initialize text-to-speech engine, explicitly specifying the espeak driver
engine = pyttsx3.init(driverName='espeak')

# Function to convert text to speech
def speak(text):
    engine.say(text)
    engine.runAndWait()

# Your existing code
from langchain_community.document_loaders import PyPDFDirectoryLoader
from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS, Chroma
from langchain_community.llms import LlamaCpp
from langchain.chains import RetrievalQA, LLMChain

import pathlib
import textwrap
from IPython.display import display, Markdown

def to_markdown(text):
    text = text.replace('â€¢', '  *')
    return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))

# Setup HuggingFace Access Token
import os
from getpass import getpass

HUGGINGFACEHUB_API_TOKEN = os.getenv("HUGGINGFACEHUB_API_TOKEN")
os.environ["HUGGINGFACEHUB_API_TOKEN"] = HUGGINGFACEHUB_API_TOKEN

# Import document
from google.colab import drive
drive.mount('/content/drive')

loader = PyPDFDirectoryLoader("/content/drive/MyDrive/Pdf")
docs = loader.load()

# Text Splitting - Chunking
text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)
chunks = text_splitter.split_documents(docs)

embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-base-en-v1.5")

# Vector Store - FAISS or ChromaDB
vectorstore = Chroma.from_documents(chunks, embeddings)

query = "Result"
search = vectorstore.similarity_search(query)
to_markdown(search[0].page_content)

# Retriever
retriever = vectorstore.as_retriever(search_kwargs={'k': 5})
retriever.get_relevant_documents(query)

# Large Language Model - Open Source
llm = LlamaCpp(
    model_path= "/content/drive/MyDrive/Pdf/Bio/BioMistral-7B.Q4_K_M.gguf",
    temperature=0.3,
    max_tokens=2048,
    top_p=1
)

# RAG Chain
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser
from langchain.prompts import ChatPromptTemplate

template = """
<|context|>
You are an AI assistant that follows instruction extremely well.
Please be truthful and give direct answers
</s>
<|user|>
{query}
</s>
 <|assistant|>
"""

prompt = ChatPromptTemplate.from_template(template)
rag_chain = (
    {"context": retriever, "query": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

# Interactive Loop
while True:
    user_input = input(f"Input Prompt: ")
    if user_input == 'exit':
        print('Exiting')
        speak('Exiting')
        sys.exit()
    if user_input == '':
        continue
    result = rag_chain.invoke(user_input)
    print("Answer: ", result)
    speak(result)

import pyttsx3
import sys
from langchain_core._api.deprecation import surface_langchain_deprecation_warnings

# Initialize text-to-speech engine, explicitly specifying the espeak driver
engine = pyttsx3.init(driverName='espeak')

# Function to convert text to speech
def speak(text):
    engine.say(text)
    engine.runAndWait()

# Your existing code
from langchain_community.document_loaders import PyPDFDirectoryLoader
from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS, Chroma
from langchain_community.llms import LlamaCpp
from langchain.chains import RetrievalQA, LLMChain

import pathlib
import textwrap
from IPython.display import display, Markdown

def to_markdown(text):
    text = text.replace('â€¢', '  *')
    return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))

# Setup HuggingFace Access Token
import os
from getpass import getpass

HUGGINGFACEHUB_API_TOKEN = os.getenv("HUGGINGFACEHUB_API_TOKEN")
os.environ["HUGGINGFACEHUB_API_TOKEN"] = HUGGINGFACEHUB_API_TOKEN

# Import document
from google.colab import drive
drive.mount('/content/drive')

loader = PyPDFDirectoryLoader("/content/drive/MyDrive/Pdf")
docs = loader.load()

# Text Splitting - Chunking
text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)
chunks = text_splitter.split_documents(docs)

embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-base-en-v1.5")

# Vector Store - FAISS or ChromaDB
vectorstore = Chroma.from_documents(chunks, embeddings)

query = "Result"
search = vectorstore.similarity_search(query)
to_markdown(search[0].page_content)

# Retriever
retriever = vectorstore.as_retriever(search_kwargs={'k': 5})
retriever.get_relevant_documents(query)

# Large Language Model - Open Source
llm = LlamaCpp(
    model_path= "/content/drive/MyDrive/Pdf/Bio/BioMistral-7B.Q4_K_M.gguf",
    temperature=0.3,
    max_tokens=2048,
    top_p=1
)

# RAG Chain
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser
from langchain.prompts import ChatPromptTemplate

template = """
<|context|>
You are an AI assistant that follows instruction extremely well.
Please be truthful and give direct answers
</s>
<|user|>
{query}
</s>
 <|assistant|>
"""

prompt = ChatPromptTemplate.from_template(template)
rag_chain = (
    {"context": retriever, "query": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

# Interactive Loop
while True:
    user_input = input(f"Input Prompt: ")
    if user_input == 'exit':
        print('Exiting')
        speak('Exiting')
        sys.exit()
    if user_input == '':
        continue
    result = rag_chain.invoke(user_input)
    print("Answer: ", result)
    speak(result)

#shell
!pip install huggingface_hub

!pip install langchain sentence-transformers chromadb llama-cpp-python langchain_community pypdf

"""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

"""

# Setup HuggingFace Access Token
import os
from getpass import getpass
HUGGINGFACEHUB_API_TOKEN = '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'
HUGGINGFACEHUB_API_TOKEN = os.getenv("HUGGINGFACEHUB_API_TOKEN")
# Check if the environment variable is set, if not prompt the user to enter it
if HUGGINGFACEHUB_API_TOKEN is None:
  HUGGINGFACEHUB_API_TOKEN = getpass("Enter your Hugging Face API token: ") # Added quotes around the API token prompt.
os.environ["HUGGINGFACEHUB_API_TOKEN"] = HUGGINGFACEHUB_API_TOKEN

"""# # Speeking AI"""

import os

# Set your Hugging Face API token directly
HUGGINGFACEHUB_API_TOKEN = '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'

# Set the environment variable
os.environ["HUGGINGFACEHUB_API_TOKEN"] = HUGGINGFACEHUB_API_TOKEN

!pip install langchain sentence-transformers chromadb llama-cpp-python langchain_community pypdf

!sudo apt-get install libespeak1
!sudo apt-get update
!sudo apt-get install espeak
!pip install pyttsx3
import pyttsx3
import sys
#from langchain_core._api.deprecation import surface_langchain_deprecation_warnings

# Initialize text-to-speech engine, explicitly specifying the espeak driver
engine = pyttsx3.init(driverName='espeak')

# Function to convert text to speech
def speak(text):
    engine.say(text)
    engine.runAndWait()

# Your existing code
from langchain_community.document_loaders import PyPDFDirectoryLoader
from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS, Chroma
from langchain_community.llms import LlamaCpp
from langchain.chains import RetrievalQA, LLMChain

import pathlib
import textwrap
from IPython.display import display, Markdown

def to_markdown(text):
    text = text.replace('â€¢', '  *')
    return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))

#from google.colab import userdata
#userdata.get('secretName')

# Setup HuggingFace Access Token
import os
from getpass import getpass

HUGGINGFACEHUB_API_TOKEN = os.getenv("HUGGINGFACEHUB_API_TOKEN")
os.environ["HUGGINGFACEHUB_API_TOKEN"] = HUGGINGFACEHUB_API_TOKEN

# Import document
from google.colab import drive
drive.mount('/content/drive')

loader = PyPDFDirectoryLoader("/content/drive/MyDrive/Pdf")
docs = loader.load()

# Text Splitting - Chunking
text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)
chunks = text_splitter.split_documents(docs)

embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-base-en-v1.5")

# Vector Store - FAISS or ChromaDB
vectorstore = Chroma.from_documents(chunks, embeddings)

query = "Result"
search = vectorstore.similarity_search(query)
to_markdown(search[0].page_content)

# Retriever
retriever = vectorstore.as_retriever(search_kwargs={'k': 5})
retriever.get_relevant_documents(query)

# Large Language Model - Open Source
llm = LlamaCpp(
    model_path= "/content/drive/MyDrive/Pdf/Bio/BioMistral-7B.Q4_K_M.gguf",
    temperature=0.3,
    max_tokens=2048,
    top_p=1
)

# RAG Chain
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser
from langchain.prompts import ChatPromptTemplate

template = """
<|context|>
You are an AI assistant that follows instruction extremely well.
Please be truthful and give direct answers
</s>
<|user|>
{query}
</s>
 <|assistant|>
"""

prompt = ChatPromptTemplate.from_template(template)
rag_chain = (
    {"context": retriever, "query": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

# # Interactive Loop
# while True:
#     user_input = input(f"Input SriRam: ")
#     if user_input == 'exit':
#         print('Exiting')
#         speak('Exiting')
#         sys.exit()
#     if user_input == '':
#         continue
#     result = rag_chain.invoke(user_input)
#     print("AI_Answer: ", result)
#     speak(result)

!pip install colorama
import sys
from colorama import Fore, Style, init

# Initialize colorama
init()

while True:
    user_input = input(Fore.YELLOW + "Input SriRam: " + Style.RESET_ALL)  # Yellow input prompt
    if user_input == 'exit': # Removed extra indentation
        print(Fore.RED + 'Exiting' + Style.RESET_ALL)  # Red 'Exiting' message
        speak('Exiting')
        sys.exit()

    if user_input == '':
        continue

    result = rag_chain.invoke(user_input)

    # Green AI answer
    print(Fore.GREEN + "AI_Answer: " + result + Style.RESET_ALL)
    speak(result)

# Interactive Loop
while True:
    user_input = input(f"Input SriRam: ")
    if user_input == 'exit':
        print('Exiting')
        speak('Exiting')
        sys.exit()
    if user_input == '':
        continue
    result = rag_chain.invoke(user_input)
    print("AI_Answer: ", result)
    speak(result)

"""# Text **AI**"""



import os

# Set your Hugging Face API token directly
HUGGINGFACEHUB_API_TOKEN = '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'

# Set the environment variable
os.environ["HUGGINGFACEHUB_API_TOKEN"] = HUGGINGFACEHUB_API_TOKEN



!pip install langchain-community # install the module
!pip install langchain sentence-transformers chromadb llama-cpp-python langchain_community pypdf
from langchain_community.document_loaders import PyPDFDirectoryLoader
from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS, Chroma
from langchain_community.llms import LlamaCpp
from langchain.chains import RetrievalQA, LLMChain

import pathlib
import textwrap
from IPython.display import display, Markdown

def to_markdown(text):
    text = text.replace('â€¢', '  *')
    return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))

# Setup HuggingFace Access Token
import os
from getpass import getpass

HUGGINGFACEHUB_API_TOKEN = os.getenv("HUGGINGFACEHUB_API_TOKEN")
os.environ["HUGGINGFACEHUB_API_TOKEN"] = HUGGINGFACEHUB_API_TOKEN

# Import document
from google.colab import drive
drive.mount('/content/drive')

loader = PyPDFDirectoryLoader("/content/drive/MyDrive/Pdf")
docs = loader.load()

# Text Splitting - Chunking
text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)
chunks = text_splitter.split_documents(docs)

embeddings = HuggingFaceEmbeddings(model_name="BAAI/bge-base-en-v1.5")

# Vector Store - FAISS or ChromaDB
vectorstore = Chroma.from_documents(chunks, embeddings)

query = "Result"
search = vectorstore.similarity_search(query)
to_markdown(search[0].page_content)

# Retriever
retriever = vectorstore.as_retriever(search_kwargs={'k': 5})
retriever.get_relevant_documents(query)

# Large Language Model - Open Source
llm = LlamaCpp(
    model_path= "/content/drive/MyDrive/Pdf/Bio/BioMistral-7B.Q4_K_M.gguf",
    temperature=0.3,
    max_tokens=2048,
    top_p=1
)

# RAG Chain
from langchain.schema.runnable import RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser
from langchain.prompts import ChatPromptTemplate

template = """
<|context|>
You are an AI assistant that follows instruction extremely well.
Please be truthful and give direct answers
</s>
<|user|>
{query}
</s>
 <|assistant|>
"""

prompt = ChatPromptTemplate.from_template(template)
rag_chain = (
    {"context": retriever, "query": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

# Interactive Loop
# while True:
#     user_input = input(f"Input Prompt: ")
#     if user_input == 'exit':
#         print('Exiting')

#         sys.exit()
#     if user_input == '':
#         continue
#     result = rag_chain.invoke(user_input)
#     print("Answer: ", result)

!pip install translate
!pip install httpcore==0.9.1 httpx==0.13.3

import sys
from translate import Translator

# Initialize the translator for English to Telugu
translator = Translator(to_lang="te")

while True:
    user_input = input(f"Inputâ”â”â•¬Ù¨Ù€ï®©ï®©â¤Ù¨Ù€ï®©ï®©Ù€â•¬â”ğ’ğ«ğ¢ğ‘ğšğ¦: ")

    if user_input.lower() == 'exit':
        print('Exiting')
        sys.exit()

    if user_input == '':
        continue

    # Translate and display user input in Telugu
    translated_input = translator.translate(user_input)
    print(f"ğ˜ğ¨ğ®ğ« ğ¢ğ§ğ©ğ®ğ­ ğ¢ğ§ ğ“ğğ¥ğ®ğ ğ®: {translated_input}")

    # Process the input with rag_chain (replace with your actual rag_chain implementation)
    result = rag_chain.invoke(user_input)

    # Translate the result to Telugu (assuming rag_chain returns the result in English)
    translated_result = translator.translate(result)
    print("Answer: ", result)
    # Print the final answer in Telugu
    print("ğ—”ğ—œ ğ—”ğ—»ğ˜€ğ˜„ğ—²ğ—¿ ğ—¶ğ—» ğ—§ğ—²ğ—¹ğ˜‚ğ—´ğ˜‚: ", translated_result)

import sys
from translate import Translator

# Initialize the translator for English to Telugu
translator = Translator(to_lang="te")

while True:
    user_input = input(f"Inputâ”â”â•¬Ù¨Ù€ï®©ï®©â¤Ù¨Ù€ï®©ï®©Ù€â•¬â”ğ’ğ«ğ¢ğ‘ğšğ¦: ")

    if user_input.lower() == 'exit':
        print('Exiting')
        sys.exit()

    if user_input == '':
        continue

    # Translate user input to Telugu
    translated_input = translator.translate(user_input)
    #print(f"Translated to Telugu: {translated_input}")

    # Process the input with rag_chain (replace with your actual rag_chain implementation)
    result = rag_chain.invoke(user_input)

    # Translate the result to Telugu (assuming rag_chain returns the result in English)
    translated_result = translator.translate(result)

    # Print the final answer in Telugu
    print("ğ—”ğ—œ ğ—”ğ—»ğ˜€ğ˜„ğ—²ğ—¿ ğ—¶ğ—» ğ—§ğ—²ğ—¹ğ˜‚ğ—´ğ˜‚: ", translated_result)

# Interactive Loop
while True:
    user_input = input(f"Input Prompt: ")
    if user_input == 'exit':
        print('Exiting')

        sys.exit()
    if user_input == '':
        continue
    result = rag_chain.invoke(user_input)
    print("Answer: ", result)